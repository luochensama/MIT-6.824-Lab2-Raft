在6.824的课程中，Lab2的任务是实现一个简易的Raft协议。分为了四个阶段：A.领导人选举 B.日志同步 C.重要状态持久化 D.快照。每个阶段都有一些Unit Test。该仓库是对Lab的总结。Test耗时如下（启用了-race）：

```shell
Test (2D): crash and restart all servers ...
  ... Passed --  13.5  3  902  199140   68
PASS
ok      6.824/raft      435.002s

real    7m15.310s
user    2m11.345s
sys     0m9.823s
```

在run10次的情况下有时可能会出现一两个Fail，主要的原因有两个：Leader选举太慢，Test中要求5s内选举出Leader，但是有时因为一些原因没那么快选举出来，就会导致没有Leader的情况。还有就是日志的提交可能超过了Test要求的10s（特别是在网络是不可信的情况下），就会出现错误。但程序的大体应该是没有问题的。

### Raft

#### 总结一下几个完成Lab时印象比较深刻的Raft的特点

这些特性简化了很多Raft的代码实现，在实现的时候需要注意

- 一：每个Term的Leader不超过一个

  不超过的意思是可能没有Leader，这种情况在多个Follower同时发起投票的时候是可能发生的，但对raft的正确性不会造成影响（前提是睡眠时间是随机的，对时间的要求见二）。

- 二：心跳广播时间 << 选举超时时间 << 平均故障时间

  这些时间的要求需要严格遵守，不然会影响整个Raft协议的性能。第一个小于保证了Leader能够在选举出来的第一时刻通知每个Peer，让这些Peer不会再次重复的发起投票（不影响正确性但影响效率）。第二个小于保证了系统大部分时候是处于可用的状态的。

- 三：每个节点机器是对等的（Peer）

  Raft中每个机器叫做一个Peer（中文：对等点），也就是意味着每个机器都有机会成为Leader（前提是日志没有落后）。

- 四：被应用的Log一定是强一致的

  当一个Log被应用到状态机的时候，一定是经过了大部分机器认证的，所以在之后的快照机制中，可以每个机器在自己想做快照的时候做快照，而不是等待Leader做完快照再发送到Follower上。

- 五：可用性随着当前系统机器的宕机量增加而降低

  因为Log的提交条件是超过半数的机器收到，当这个数量越来越少时，就变成了需要全部机器收到才能提交，如果超过半数都宕机或者是网络不通，那么整个raft协议都无法正常运作。

- 六：当收到大于自己的Term的RPC请求的时候将自己切换为Follower

  这个点说着很简单，但是却很重要。考虑一个场景：一个Peer延迟了很久很久，他自己可能发起了N轮投票，突然链接回去了，那么这时候原先正常工作的机器就会收到该Peer的RequestVote，收到的时候，发现Term大于自己，那么就切换为Follower，重新选举。这样处理似乎效率很低，但其实一般机器也不会频繁的出现故障，所以是可以忍受的。这样的特性保证了Raft的正确性。
  
- 七：发送HeartBeat的时候的策略

  在编写代码的时候思考过几种发送心跳的策略：

  将心跳和日志同步放在一起实现。这个策略会导致一个问题，就是心跳是每隔一段时间就要发送的，但是日志的同步并不是。如果日志的同步在一对一的时候是并行的，会带来很多编码上的烦恼。但不是并行的话就会导致心跳不能及时到达（因为要等待Follower回应）。所以需要将他们分开。

  将心跳和日志同步分开实现。有一个线程专门负责心跳RPC（每隔一段时间发送），另一个线程负责日志同步（需要发送的时候发送）。这样就能在保证心跳的同时串行的实现点对点的日志传送。

- 八：日志同步实现的策略

  一开始的策略（以一个Follower为例）：CallAppendEntriesRPC -> 等待 -> 处理Reply，这个策略一开始是没什么问题的，但是到了2D的时候发现如果Follower宕机重启或者是网络不通的时候这个RPC调用超时可能会比重启时间或者是网络调整时间更大。在实际使用中不知道这个问题大不大，但是过不了2D的某些Test。

  之后的“优化”策略，这个优化打上了引号是因为我也不好肯定一定比之前的策略就好，反而可能有一些新的问题出现，但是能过Lab的Test。具体调整如下：

  CallAppendEntriesRPC -> 等待一段时间 -> 处理Reply或者重新发送RPC请求。也就是不等到RPC返回，就直接发送。这样的策略能够有效的处理上面提出的问题，但是会导致Follower收到重复的Log。

- 九：快照实现的策略

  快照机制会导致的一个比较严重的问题是Index被分裂成了当前Log中的Index和全局的Index。因为每个机器的Snapshot可能都不一样。所以出现了LogIndex和GlobalIndex，GlobalIndex可以保障Raft的正确性，但是对编程的影响很大。

  在我的实现里是将需要"交流"的时候将LogIndex转为GlobalIndex，在自己的应用中使用LogIndex。代码细节较多，不表。

